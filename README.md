# LTL Reward Specification in a CMDP
This repository is for my Master's Thesis. I am working under the supervision of Professor Yves Lesperance. 

## Abstract
Reinforcement Learning (RL) is a field of machine learning that focuses in training intelligent agents on how it ought to act in order to achieve a set task in an unknown environment. The environment is typically assumed to be an unknown of a Markov decision process (MDP). Reinforcement Learning algorithms train an agent by making it explore the unknown environment (explo- ration) and learn which actions will maximize the cumulative reward provided by said environment (exploitation). This thesis will focus on improving two of the main concerns with reinforcement learning, safety and performance. 

When training an agent in a MDP its main focus is to maximize the cumu- lative reawrd itreceives. So, the exploration process typically does not concern itself with potential threats to the agent and therefore can not guarantee its safety. Constrained Markov Decision Processes are a special form of MDPs that allow for the separation of safety specifications from the reward function by naturally encoding the safety concerns as constraints. They were first intro- duced by Altman [1999]. A CMDP will have the same components as an MDP plus a cost function and an upper bound for the expected cumulative constraint cost. In a CMDP the goal is to learn an optimal policy that maximizes the cumulative reward and who's cumulative cost is under the established upper bound. 

In addition, intelligent agents do not have access to the reward function governing their environment. For the agents, the reward function is a black- box that will return an immediate reward given their current state. Therefore, finding an optimal policy using reinforcement learning can require a lot of exploration of the environment. Sometimes this can be of very high cost and in some cases not possible to perform, such as in physical environments. So, if the goal is for the agent to maximize cumulative reward, it would be helpful if they could have any knowledge of their governing reward function/s. One way to approach this limitation is by introducing the use of logic to supply prior knowledge to the agent. Icarte et al. [2018] introduced Reward Machines (RM), a type of finite state machine that would allow the agent to be exposed to the structure of the reward function. The idea is for the RM to replace the original reward function in an MDP, transforming the MDP into a MDP with a Reward Machine (MDPRM). The MDPRM can then be exploited by a RL algorithm in a way that it will allow the agent to decompose their task and speed up the learning process. RMs can have some limitations in its ability to specify every type of reward-worthy behaviour. Instead one can use formal languages, such as Linear Temporal Logic (LTL), for this part of the process. Then, the reward machines can be constructed from said formal specification.

For my thesis, I propose the integration of Reward Machines into Con- strained Markov Decision Processes. The goal is to develop a safer and more efficient solution to the constrained RL control problem. The ideas is to use Linear Temporal Logic (LTL) to translate the reward functions into formal rewards and use it to construct an equivalent Reward Machine. I will be using the RM to transform the CMDP into a new framework called CMDP with a Reward Machine (CMDPRM) and, to solve the CMDPRM I will be developing a new algorithm called Constrained Learning for RM (CLRM) that will lever- age the RM to achieve a faster and safer learning process. The goal is to test and evaluate the proposed methodology in the Safety Gym benchmark suite developed by Ray et al. [2019], which consists of high-dimensional continuous control environments meant to measure the performance and safety of agents in Constrained Markov Decision Processes. 

## References

Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.

Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. Using reward machines for high-level task specification and decomposition in reinforcement learning. In *International Conference on Machine Learning*, pages 2107-2116, 2018. 

Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement learning. *arXiv preprint arXiv:1910.01708*, 2019.
